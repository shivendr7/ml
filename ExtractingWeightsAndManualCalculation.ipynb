{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtractingWeightsAndManualCalculation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbVfxmgLvzpelS6WhuWxQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivendr7/ml/blob/main/ExtractingWeightsAndManualCalculation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZSc14baYCQV"
      },
      "source": [
        "By defining the center and how large the standard deviations are, you are able to control the range of random numbers that you will receive.\r\n",
        "\r\n",
        "The Xavier weight initialization sets all of the weights to normally distributed random numbers. These weights are always centered at 0; however, their standard deviation varies depending on how many connections are present for the current layer of weights. Specifically, Equation 4.2 can determine the standard deviation:\r\n",
        "\r\n",
        "Var(W)=2/(nin+nout)\r\n",
        "\r\n",
        "nin=number of neurons in layer i-1\r\n",
        "\r\n",
        "nout=no. of neurons in layer i \r\n",
        "\r\n",
        "The above equation shows how to obtain the variance for all of the weights. The square root of the variance is the standard deviation. Most random number generators accept a standard deviation rather than a variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mth8akcXgaq"
      },
      "source": [
        "\"\"\"\r\n",
        " We will train a simple neural network that learns the XOR function. \r\n",
        " It is not hard to simply hand-code the neurons to provide an XOR function; \r\n",
        " however, for simplicity, we will allow Keras to train this network for us. \r\n",
        " We will just use 100K epochs on the ADAM optimizer. This is massive overkill,\r\n",
        "  but it gets the result, and our focus here is not on tuning. The neural network\r\n",
        " is small. Two inputs, two hidden neurons, and a single output.\r\n",
        "\"\"\"\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Activation\r\n",
        "import numpy as np"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyOgIupnYzlL",
        "outputId": "652ea528-4933-49c1-933e-75be33b07d38"
      },
      "source": [
        "x=np.array([\r\n",
        "            [0,0],\r\n",
        "            [1,0],\r\n",
        "            [0,1],\r\n",
        "            [1,1]\r\n",
        "])\r\n",
        "y=np.array([\r\n",
        "            0,\r\n",
        "            1,\r\n",
        "            1,\r\n",
        "            0\r\n",
        "])\r\n",
        "# Build the network\r\n",
        "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\n",
        "\r\n",
        "done=False\r\n",
        "cycle=1\r\n",
        "#np.set_printoptions(suppress=True)\r\n",
        "\r\n",
        "while not done:\r\n",
        "  print(\"Cycle #{}\".format(cycle))\r\n",
        "  cycle+=1\r\n",
        "  model=Sequential()\r\n",
        "  model.add(Dense(2, input_dim=2, activation='relu'))\r\n",
        "  model.add(Dense(1))\r\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\r\n",
        "  model.fit(x, y, verbose=0, epochs=10000)\r\n",
        "\r\n",
        "  pred=model.predict(x)\r\n",
        "\r\n",
        "  #check success\r\n",
        "  done= pred[0]<0.01 and pred[3]<.01 and pred[1]>0.9 and pred[2]>.9\r\n",
        "  print(pred)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cycle #1\n",
            "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9cd7c2f320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "[[-0.00000046]\n",
            " [ 0.99999934]\n",
            " [ 0.9999992 ]\n",
            " [-0.00000055]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_td5-ZndDuH",
        "outputId": "95f2feff-85ce-40a1-8602-f2326dfdb604"
      },
      "source": [
        "#Extracting weights\r\n",
        "for layerNum, layer in enumerate(model.layers):\r\n",
        "  weights=layer.get_weights()[0]\r\n",
        "  biases=layer.get_weights()[1]\r\n",
        "  print(weights,'\\n',biases)\r\n",
        "  for toNeuronNum, bias in enumerate(biases):\r\n",
        "    print(f'{layerNum}B -> L{layerNum+1}N{toNeuronNum}:{bias}')\r\n",
        "  print()\r\n",
        "  for fromNeuronNum, wgt in enumerate(weights):\r\n",
        "    for  toNeuronNum, wgt2 in enumerate(wgt):\r\n",
        "      print(f'L{layerNum}N{fromNeuronNum} -> L{layerNum}N{toNeuronNum} = {wgt2}')\r\n",
        "  print()\r\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.2098621   0.6771087 ]\n",
            " [ 0.6269057  -0.67710876]] \n",
            " [-0.00021322  0.00636576]\n",
            "0B -> L1N0:-0.00021321882377378643\n",
            "0B -> L1N1:0.0063657574355602264\n",
            "\n",
            "L0N0 -> L0N0 = -1.2098621129989624\n",
            "L0N0 -> L0N1 = 0.6771087050437927\n",
            "L0N1 -> L0N0 = 0.6269056797027588\n",
            "L0N1 -> L0N1 = -0.6771087646484375\n",
            "\n",
            "[[1.6106801]\n",
            " [1.4768674]] \n",
            " [-0.00940184]\n",
            "1B -> L2N0:-0.009401840157806873\n",
            "\n",
            "L1N0 -> L1N0 = 1.610680103302002\n",
            "L1N1 -> L1N0 = 1.476867437362671\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOClulZDmqLU",
        "outputId": "7f858813-a2fb-4c71-8eca-1c679a7c7474"
      },
      "source": [
        "#using weights\r\n",
        "input0 = 0\r\n",
        "input1 = 1\r\n",
        "\r\n",
        "hidden0Sum = (input0*1.3)+(input1*1.3)+(-1.3)\r\n",
        "hidden1Sum = (input0*1.2)+(input1*1.2)+(0)\r\n",
        "\r\n",
        "print(hidden0Sum) # 0\r\n",
        "print(hidden1Sum) # 1.2\r\n",
        "\r\n",
        "hidden0 = max(0,hidden0Sum)\r\n",
        "hidden1 = max(0,hidden1Sum)\r\n",
        "\r\n",
        "print(hidden0) # 0\r\n",
        "print(hidden1) # 1.2\r\n",
        "\r\n",
        "outputSum = (hidden0*-1.6)+(hidden1*0.8)+(0)\r\n",
        "print(outputSum) # 0.96\r\n",
        "\r\n",
        "output = max(0,outputSum)\r\n",
        "\r\n",
        "print(output) # 0.96"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "1.2\n",
            "0\n",
            "1.2\n",
            "0.96\n",
            "0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}